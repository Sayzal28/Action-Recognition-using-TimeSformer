{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ln8QSkwKNjVT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ln8QSkwKNjVT",
    "outputId": "67339ffd-c4a4-4b67-831c-5350e7b572c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae16ed2c-0fad-4476-ba1b-6b0af1d79b4c",
   "metadata": {
    "id": "ae16ed2c-0fad-4476-ba1b-6b0af1d79b4c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "project_path = '/content/drive/MyDrive/action-recognition-vit'  # Update with your project path\n",
    "os.chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9778f660-e086-47c1-a13e-412a6cc77e05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "9778f660-e086-47c1-a13e-412a6cc77e05",
    "outputId": "963022ce-f4de-49e1-b772-21e6571947af",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.21.0+cu124)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (4.54.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (2.0.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.12.0.88)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.18.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (11.3.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (0.13.2)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask->-r requirements.txt (line 1)) (8.2.1)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (0.34.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 8)) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 8)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 9)) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 9)) (1.74.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 9)) (3.8.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 9)) (5.29.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 9)) (75.2.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 9)) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 9)) (0.7.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn->-r requirements.txt (line 11)) (2.2.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 4)) (1.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 11)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 11)) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2025.7.14)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5oNUM7xJOJxo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oNUM7xJOJxo",
    "outputId": "32ba4bb9-277f-49fe-9cb4-187c065ccf28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted to /content/drive/MyDrive/action-recognition-vit\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "# Unzip dataset\n",
    "zip_path = '/content/drive/MyDrive/action-recognition-vit/HMDB_simp.zip'\n",
    "extract_path = '/content/drive/MyDrive/action-recognition-vit'\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Dataset extracted to {extract_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a251f11f-5e30-4ae0-b3cb-b172ef819f02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a251f11f-5e30-4ae0-b3cb-b172ef819f02",
    "outputId": "9e73b3cb-7263-482a-b041-2504a6777b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found!\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '/content/drive/MyDrive/action-recognition-vit/HMDB_simp'  # Update with your dataset path\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
    "else:\n",
    "    print(\"Dataset found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cd85e15-87ab-4e13-9885-cfd29d59c7cf",
   "metadata": {
    "id": "9cd85e15-87ab-4e13-9885-cfd29d59c7cf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.trainer_utils import IntervalStrategy\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Suppress FutureWarnings\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast  # For mixed precision training\n",
    "import logging\n",
    "import sys\n",
    "import argparse\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForVideoClassification\n",
    "import os\n",
    "from transformers import TimesformerForVideoClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "087a0b37-d1d3-44ad-ac1c-88fda4ba3676",
   "metadata": {
    "id": "087a0b37-d1d3-44ad-ac1c-88fda4ba3676"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MultiClipHMDBDataset(Dataset):\n",
    "    def __init__(self, root_dir, clip_size=8, frame_rate=32, transform=None):\n",
    "        \"\"\"\n",
    "        Virtual dataset that creates multiple clips per video with frame augmentation\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): Path to dataset directory\n",
    "            clip_size (int): Number of frames per clip\n",
    "            frame_rate (int): Sampling rate (every Nth frame)\n",
    "            transform: Torchvision transforms\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.clip_size = clip_size\n",
    "        self.frame_rate = frame_rate\n",
    "        self.transform = transform\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _augment_frames_list(self, frames_list):\n",
    "        \"\"\"Augment frames when we have fewer than clip_size\"\"\"\n",
    "        if len(frames_list) == 0:\n",
    "            # Create black frame paths\n",
    "            return ['black_frame'] * self.clip_size\n",
    "\n",
    "        elif len(frames_list) == 1:\n",
    "            # Repeat the single frame\n",
    "            return frames_list * self.clip_size\n",
    "\n",
    "        else:\n",
    "            # Frame interpolation and temporal reversal\n",
    "            augmented_frames = []\n",
    "\n",
    "            # Add original frames\n",
    "            for i, frame in enumerate(frames_list):\n",
    "                augmented_frames.append(frame)\n",
    "\n",
    "                # Add interpolated frame between consecutive frames\n",
    "                if i < len(frames_list) - 1:\n",
    "                    augmented_frames.append(f\"interp_{frame}_{frames_list[i+1]}\")\n",
    "\n",
    "            # If still not enough, use temporal reversal\n",
    "            while len(augmented_frames) < self.clip_size:\n",
    "                reversed_frames = augmented_frames[::-1]\n",
    "                augmented_frames.extend(reversed_frames)\n",
    "\n",
    "            return augmented_frames[:self.clip_size]\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Create virtual clips from videos with multiple clips per video\"\"\"\n",
    "        data = []\n",
    "        subfolders = sorted(os.listdir(self.root_dir))\n",
    "\n",
    "        if len(subfolders) != 25:\n",
    "            raise ValueError(f\"Dataset must contain exactly 25 subfolders. Found {len(subfolders)}.\")\n",
    "\n",
    "        for label, action in enumerate(subfolders):\n",
    "            action_path = os.path.join(self.root_dir, action)\n",
    "            if not os.path.isdir(action_path):\n",
    "                continue\n",
    "\n",
    "            for video_folder in os.listdir(action_path):\n",
    "                video_path = os.path.join(action_path, video_folder)\n",
    "                if not os.path.isdir(video_path):\n",
    "                    continue\n",
    "\n",
    "                # Skip already processed folders\n",
    "                if '_' in video_folder and video_folder.split('_')[-1].isdigit():\n",
    "                    continue\n",
    "\n",
    "                # Get image frames\n",
    "                all_frames = sorted([f for f in os.listdir(video_path)\n",
    "                                   if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
    "\n",
    "                if len(all_frames) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Sample frames\n",
    "                sampled_frames = all_frames[::self.frame_rate]\n",
    "\n",
    "                # Create mutually exclusive clips (no overlapping frames)\n",
    "                clips_created = 0\n",
    "\n",
    "                if len(sampled_frames) >= self.clip_size:\n",
    "                    # Calculate how many complete clips we can make\n",
    "                    num_complete_clips = len(sampled_frames) // self.clip_size\n",
    "\n",
    "                    if num_complete_clips >= 2:\n",
    "                        # Create multiple non-overlapping clips\n",
    "                        for clip_idx in range(num_complete_clips):\n",
    "                            start_idx = clip_idx * self.clip_size\n",
    "                            end_idx = start_idx + self.clip_size\n",
    "                            clip_frames = sampled_frames[start_idx:end_idx]\n",
    "\n",
    "                            data.append((video_path, label, clips_created, clip_frames))\n",
    "                            clips_created += 1\n",
    "\n",
    "                    else:\n",
    "                        # Only enough frames for 1 complete clip (8-15 frames)\n",
    "                        # Take the first clip_size frames\n",
    "                        clip_frames = sampled_frames[:self.clip_size]\n",
    "                        data.append((video_path, label, clips_created, clip_frames))\n",
    "                        clips_created += 1\n",
    "\n",
    "                else:\n",
    "                    # Use augmentation for insufficient frames (<8 frames)\n",
    "                    augmented_frames = self._augment_frames_list(sampled_frames)\n",
    "                    data.append((video_path, label, clips_created, augmented_frames))\n",
    "                    clips_created += 1\n",
    "\n",
    "                # Calculate remaining frames that were not used\n",
    "                frames_used = min(len(sampled_frames), (len(sampled_frames) // self.clip_size) * self.clip_size)\n",
    "                frames_discarded = len(sampled_frames) - frames_used\n",
    "\n",
    "\n",
    "        print(f\"Total clips created: {len(data)}\")\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _load_frames_from_clip(self, video_path, clip_frames):\n",
    "        \"\"\"Load frames for a specific clip\"\"\"\n",
    "        frames = []\n",
    "\n",
    "        for frame_name in clip_frames:\n",
    "            if frame_name == 'black_frame':\n",
    "                # Create black frame\n",
    "                img = Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "\n",
    "            elif frame_name.startswith('interp_'):\n",
    "                # Handle interpolated frames\n",
    "                # Format: \"interp_frame1_frame2\"\n",
    "                parts = frame_name.replace('interp_', '').split('_')\n",
    "                if len(parts) >= 2:\n",
    "                    frame1_name = '_'.join(parts[:-1])\n",
    "                    frame2_name = parts[-1]\n",
    "\n",
    "                    frame1_path = os.path.join(video_path, frame1_name)\n",
    "                    frame2_path = os.path.join(video_path, frame2_name)\n",
    "\n",
    "                    if os.path.exists(frame1_path) and os.path.exists(frame2_path):\n",
    "                        try:\n",
    "                            img1 = np.array(Image.open(frame1_path))\n",
    "                            img2 = np.array(Image.open(frame2_path))\n",
    "                            # Simple interpolation\n",
    "                            img_avg = np.mean([img1, img2], axis=0, dtype=np.uint8)\n",
    "                            img = Image.fromarray(img_avg)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error interpolating frames: {e}\")\n",
    "                            img = Image.open(frame1_path)  # Fallback to first frame\n",
    "                    else:\n",
    "                        # Fallback to any available frame\n",
    "                        available_frames = [f for f in os.listdir(video_path)\n",
    "                                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "                        if available_frames:\n",
    "                            img = Image.open(os.path.join(video_path, available_frames[0]))\n",
    "                        else:\n",
    "                            img = Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "                else:\n",
    "                    # Fallback\n",
    "                    available_frames = [f for f in os.listdir(video_path)\n",
    "                                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "                    if available_frames:\n",
    "                        img = Image.open(os.path.join(video_path, available_frames[0]))\n",
    "                    else:\n",
    "                        img = Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "\n",
    "            else:\n",
    "                # Regular frame\n",
    "                frame_path = os.path.join(video_path, frame_name)\n",
    "                if os.path.exists(frame_path):\n",
    "                    try:\n",
    "                        img = Image.open(frame_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading frame {frame_path}: {e}\")\n",
    "                        img = Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "                else:\n",
    "                    print(f\"Frame not found: {frame_path}\")\n",
    "                    img = Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "\n",
    "            frames.append(img)\n",
    "\n",
    "        # Apply brightness variation for repeated frames\n",
    "        frame_names_unique = list(set([f for f in clip_frames if not f.startswith('interp_')]))\n",
    "        if len(frame_names_unique) == 1 and frame_names_unique[0] != 'black_frame':\n",
    "            # All frames are the same, apply brightness variation\n",
    "            varied_frames = []\n",
    "            for i, frame in enumerate(frames):\n",
    "                brightness_factor = 0.8 + (i % 5) * 0.1\n",
    "                frame_np = np.asarray(frame).astype(np.float32)\n",
    "                frame_np = frame_np * brightness_factor\n",
    "                frame_np = np.clip(frame_np, 0, 255).astype(np.uint8)\n",
    "                varied_frames.append(Image.fromarray(frame_np))\n",
    "            frames = varied_frames\n",
    "\n",
    "        # Ensure exactly clip_size frames\n",
    "        while len(frames) < self.clip_size:\n",
    "            if frames:\n",
    "                frames.append(frames[-1])  # Repeat last frame\n",
    "            else:\n",
    "                frames.append(Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8)))\n",
    "\n",
    "        frames = frames[:self.clip_size]\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label, clip_idx, clip_frames = self.data[idx]\n",
    "\n",
    "        # Load frames for this specific clip\n",
    "        frames = self._load_frames_from_clip(video_path, clip_frames)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            frames = [self.transform(frame) for frame in frames]\n",
    "\n",
    "        return torch.stack(frames), label\n",
    "\n",
    "def get_dataloader(root_dir, batch_size=8, clip_size=8, train_ratio=0.8, val_ratio=0.1, frame_rate=32):\n",
    "    \"\"\"\n",
    "    Create dataloaders using virtual multi-clip dataset with augmentation\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create virtual dataset\n",
    "    dataset = MultiClipHMDBDataset(\n",
    "        root_dir=root_dir,\n",
    "        clip_size=clip_size,\n",
    "        frame_rate=frame_rate,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(\"No valid clips found in dataset. Check your data structure and frame_rate parameter.\")\n",
    "\n",
    "    # Create splits\n",
    "    indices = range(len(dataset))\n",
    "    labels = [label for _, label, _, _ in dataset.data]\n",
    "\n",
    "    # Check if we have enough samples for each class\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    min_count = np.min(counts)\n",
    "    print(f\"Minimum samples per class: {min_count}\")\n",
    "\n",
    "    if min_count < 2:\n",
    "        print(\"Some classes have only 1 sample. Using random split instead of stratified split.\")\n",
    "        # First split: train+val vs test\n",
    "        train_val_idx, test_idx = train_test_split(\n",
    "            indices, test_size=1-(train_ratio+val_ratio), random_state=42\n",
    "        )\n",
    "\n",
    "        # Second split: train vs val\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            train_val_idx, test_size=val_ratio/(train_ratio+val_ratio), random_state=42\n",
    "        )\n",
    "    else:\n",
    "        # First split: train+val vs test\n",
    "        train_val_idx, test_idx = train_test_split(\n",
    "            indices, test_size=1-(train_ratio+val_ratio), stratify=labels, random_state=42\n",
    "        )\n",
    "\n",
    "        # Second split: train vs val\n",
    "        train_val_labels = [labels[i] for i in train_val_idx]\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            train_val_idx, test_size=val_ratio/(train_ratio+val_ratio),\n",
    "            stratify=train_val_labels, random_state=42\n",
    "        )\n",
    "\n",
    "    # Create subsets\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=0, pin_memory=False\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=False\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=False\n",
    "    )\n",
    "\n",
    "    print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def analyze_dataset_statistics(dataset):\n",
    "    \"\"\"\n",
    "    Print detailed statistics about the dataset showing multiple clips per video\n",
    "    \"\"\"\n",
    "    print(\"=== DATASET STATISTICS ===\")\n",
    "    print(f\"Total clips: {len(dataset)}\")\n",
    "\n",
    "    # Analyze by video and action class\n",
    "    video_clips = {}\n",
    "    action_stats = {}\n",
    "\n",
    "    for video_path, label, clip_idx, clip_frames in dataset.data:\n",
    "        action_name = os.path.basename(os.path.dirname(video_path))\n",
    "        video_name = os.path.basename(video_path)\n",
    "\n",
    "        # Count clips per video\n",
    "        video_key = f\"{action_name}/{video_name}\"\n",
    "        if video_key not in video_clips:\n",
    "            video_clips[video_key] = 0\n",
    "        video_clips[video_key] += 1\n",
    "\n",
    "        # Count clips per action\n",
    "        if action_name not in action_stats:\n",
    "            action_stats[action_name] = {'clips': 0, 'videos': set()}\n",
    "        action_stats[action_name]['clips'] += 1\n",
    "        action_stats[action_name]['videos'].add(video_name)\n",
    "\n",
    "    print(\"\\n=== CLIPS PER VIDEO DISTRIBUTION ===\")\n",
    "    clips_per_video_dist = {}\n",
    "    for video, clip_count in video_clips.items():\n",
    "        if clip_count not in clips_per_video_dist:\n",
    "            clips_per_video_dist[clip_count] = 0\n",
    "        clips_per_video_dist[clip_count] += 1\n",
    "\n",
    "    for clip_count in sorted(clips_per_video_dist.keys()):\n",
    "        video_count = clips_per_video_dist[clip_count]\n",
    "        print(f\"{clip_count} clips per video: {video_count} videos\")\n",
    "\n",
    "    print(\"\\n=== BY ACTION CLASS ===\")\n",
    "    for action, stats in sorted(action_stats.items()):\n",
    "        avg_clips_per_video = stats['clips'] / len(stats['videos']) if stats['videos'] else 0\n",
    "        print(f\"{action}: {stats['clips']} clips from {len(stats['videos'])} videos (avg: {avg_clips_per_video:.1f} clips/video)\")\n",
    "\n",
    "    print(f\"\\n=== SUMMARY ===\")\n",
    "    total_videos = len(video_clips)\n",
    "    total_clips = len(dataset)\n",
    "    avg_clips_per_video = total_clips / total_videos if total_videos > 0 else 0\n",
    "    print(f\"Total videos: {total_videos}\")\n",
    "    print(f\"Total clips: {total_clips}\")\n",
    "    print(f\"Average clips per video: {avg_clips_per_video:.2f}\")\n",
    "\n",
    "    # Show some examples of multi-clip videos\n",
    "    multi_clip_videos = [(v, c) for v, c in video_clips.items() if c > 1]\n",
    "    if multi_clip_videos:\n",
    "        print(f\"\\n=== EXAMPLES OF MULTI-CLIP VIDEOS ===\")\n",
    "        for video, clip_count in sorted(multi_clip_videos, key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"{video}: {clip_count} clips\")\n",
    "\n",
    "    return video_clips, action_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0a4aeef-d457-4b0c-a344-27eca305de67",
   "metadata": {
    "id": "a0a4aeef-d457-4b0c-a344-27eca305de67"
   },
   "outputs": [],
   "source": [
    "def load_timesformer_model():\n",
    "    \"\"\"\n",
    "    Load the pre-trained TimeSformer model for video classification.\n",
    "\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Some weights of TimesformerForVideoClassification were not initialized\")\n",
    "    logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "    label_index_dict={'brush_hair': 0, 'cartwheel': 1, 'catch': 2, 'chew': 3, 'climb': 4, 'climb_stairs': 5, 'draw_sword': 6, 'eat': 7, 'fencing': 8, 'flic_flac': 9, 'golf': 10, 'handstand': 11, 'kiss': 12, 'pick': 13, 'pour': 14, 'pullup': 15, 'pushup': 16, 'ride_bike': 17, 'shoot_bow': 18, 'shoot_gun': 19, 'situp': 20, 'smile': 21, 'smoke': 22, 'throw': 23, 'wave': 24}\n",
    "    index_label_dict={0: 'brush_hair', 1: 'cartwheel', 2: 'catch', 3: 'chew', 4: 'climb', 5: 'climb_stairs', 6: 'draw_sword', 7: 'eat', 8: 'fencing', 9: 'flic_flac',10: 'golf', 11: 'handstand', 12: 'kiss', 13: 'pick', 14: 'pour', 15: 'pullup', 16: 'pushup', 17: 'ride_bike', 18: 'shoot_bow', 19: 'shoot_gun', 20: 'situp', 21: 'smile', 22: 'smoke', 23: 'throw', 24: 'wave'}\n",
    "    # Load the processor and model from Hugging Face\n",
    "    processor =AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k600\")\n",
    "    ckpt = \"facebook/timesformer-base-finetuned-k600\"\n",
    "    model = TimesformerForVideoClassification.from_pretrained(ckpt,label2id = label_index_dict,id2label = index_label_dict,ignore_mismatched_sizes = True)\n",
    "    #model.classifier = torch.nn.Linear(model.config.hidden_size, 25)\n",
    "    # Optionally load fine-tuned weights if available\n",
    "    checkpoint_path = \"/content/drive/MyDrive/action-recognition-vit/timesformer_model.pth\"  # Update this path if you have fine-tuned weights\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
    "        print(\"Loaded fine-tuned weights from:\", checkpoint_path)\n",
    "    else:\n",
    "        print(\"Using pre-trained TimeSformer weights.\")\n",
    "\n",
    "    return processor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8806807-93ca-4dc7-a3b4-fb12994cdc4c",
   "metadata": {
    "id": "c8806807-93ca-4dc7-a3b4-fb12994cdc4c"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    if isinstance(predictions, torch.Tensor):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "    # Top-1 accuracy\n",
    "    top1_predictions = np.argmax(predictions, axis=1)\n",
    "    top1_accuracy = np.mean(top1_predictions == labels)\n",
    "\n",
    "    # Top-5 accuracy\n",
    "    top5_predictions = np.argsort(predictions, axis=1)[:, -5:]\n",
    "    top5_correct = np.array([labels[i] in top5_predictions[i] for i in range(len(labels))])\n",
    "    top5_accuracy = np.mean(top5_correct)\n",
    "\n",
    "    return {\n",
    "        \"top1_accuracy\": top1_accuracy,\n",
    "        \"top5_accuracy\": top5_accuracy,\n",
    "        \"eval_top1_accuracy\": top1_accuracy,\n",
    "        \"eval_top5_accuracy\": top5_accuracy,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22d8cfba-d4cd-438d-ba57-31e1da70f980",
   "metadata": {
    "id": "22d8cfba-d4cd-438d-ba57-31e1da70f980"
   },
   "outputs": [],
   "source": [
    "def train_model_with_trainer(data_dir, epochs, batch_size, learning_rate):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_dataloader(data_dir, batch_size)\n",
    "    processor, model = load_timesformer_model()\n",
    "\n",
    "    train_dataset = train_loader.dataset\n",
    "    val_dataset = val_loader.dataset\n",
    "\n",
    "    def data_collator(batch):\n",
    "        videos = torch.stack([item[0] for item in batch])\n",
    "        labels = torch.tensor([item[1] for item in batch])\n",
    "        return {\n",
    "            'pixel_values': videos,\n",
    "            'labels': labels\n",
    "        }\n",
    "    optimizer = optim.SGD(model.parameters(), momentum= 0.9, weight_decay= 1e-3,\n",
    "    \t\t\t  lr= learning_rate)\n",
    "\n",
    "    training_args = TrainingArguments(output_dir = \"./results\",\n",
    "                                      overwrite_output_dir = True,\n",
    "                                      eval_strategy = 'epoch',\n",
    "                                      per_device_train_batch_size = 8,\n",
    "                                      per_device_eval_batch_size = 8,\n",
    "                                      num_train_epochs = epochs,\n",
    "                                      logging_dir = os.path.normpath(os.path.join('./results', 'logs')),\n",
    "                                      logging_strategy = \"epoch\",\n",
    "                                      save_strategy = 'epoch',\n",
    "                                      save_total_limit = 1,\n",
    "                                      remove_unused_columns = False,\n",
    "                                      load_best_model_at_end = True,\n",
    "                                      metric_for_best_model = 'eval_top1_accuracy',\n",
    "                                      greater_is_better = True,\n",
    "                                      label_smoothing_factor = 0.1,\n",
    "                                      report_to = \"tensorboard\",\n",
    "                                      push_to_hub = False,\n",
    "                                      save_only_model= True)\n",
    "\n",
    "\n",
    "    early_stopping_callback = EarlyStoppingCallback(early_stopping_patience= 5 )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        optimizers=(optimizer,None),\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping_callback]\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_dataset = test_loader.dataset\n",
    "    test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "    print(f\"\\n=== FINAL TEST RESULTS ===\")\n",
    "    print(f\"Test Top-1 Accuracy: {test_results.get('eval_top1_accuracy', 'N/A'):.4f}\")\n",
    "    print(f\"Test Top-5 Accuracy: {test_results.get('eval_top5_accuracy', 'N/A'):.4f}\")\n",
    "    print(f\"Test Loss: {test_results.get('eval_loss', 'N/A'):.4f}\")\n",
    "\n",
    "    trainer.save_model(\"./timesformer_model\")\n",
    "\n",
    "    with open(\"./test_results.json\", \"w\") as f:\n",
    "        json.dump(test_results, f, indent=2)\n",
    "\n",
    "    print(\"Training complete. Model and results saved.\")\n",
    "    return test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a34459d-67f8-4e3f-8f90-b54020c63618",
   "metadata": {
    "id": "7a34459d-67f8-4e3f-8f90-b54020c63618"
   },
   "outputs": [],
   "source": [
    "def evaluate_saved_model(model_path, data_dir, batch_size=8):\n",
    "    from transformers import TimesformerForVideoClassification\n",
    "\n",
    "    model = TimesformerForVideoClassification.from_pretrained(model_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    _, _, test_loader = get_dataloader(data_dir, batch_size)\n",
    "\n",
    "    def data_collator(batch):\n",
    "        videos = torch.stack([item[0] for item in batch])\n",
    "        labels = torch.tensor([item[1] for item in batch])\n",
    "        return {\n",
    "            'pixel_values': videos,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    results = trainer.evaluate(eval_dataset=test_loader.dataset)\n",
    "\n",
    "    print(f\"=== Model Evaluation: {model_path} ===\")\n",
    "    print(f\"Top-1 Accuracy: {results.get('eval_top1_accuracy', 'N/A'):.4f}\")\n",
    "    print(f\"Top-5 Accuracy: {results.get('eval_top5_accuracy', 'N/A'):.4f}\")\n",
    "    print(f\"Loss: {results.get('eval_loss', 'N/A'):.4f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0dd14848-14ae-4176-988e-af24365795ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dd14848-14ae-4176-988e-af24365795ad",
    "outputId": "d135aab7-baa4-4ebe-bacd-3d3e2a5eb803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total clips created: 1260\n",
      "Minimum samples per class: 50\n",
      "Dataset splits: Train=1007, Val=127, Test=126\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloader(\n",
    "    root_dir=\"/content/drive/MyDrive/action-recognition-vit/HMDB_simp\",\n",
    "    batch_size=8,\n",
    "    clip_size=8,\n",
    "    frame_rate=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1923fc5-8ad9-4a05-8db9-e794b29d5e97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868,
     "referenced_widgets": [
      "d12a6ad8d6ca4858bca786d244156fa5",
      "e363cbca60394b57a51911a83b5a21b0",
      "9bab4f908db94f2eb05ca675ced2c4d2",
      "26e75539eff64aa28615f48416c4375a",
      "9d6578408e744c219653d00e71f257ad",
      "7ec4203f2ee6456baf29b8ded748c5d8",
      "5a7d6eb7951d418bb938e9fee63e556d",
      "b6959653d57341269a75d4266b6dc9ca",
      "7f1f56e383484946a401535d4613700c",
      "cbc533729c1d4142b5f74c35ca1f4f22",
      "699626548c9e4daaaacfbf2879b9b0dd"
     ]
    },
    "id": "e1923fc5-8ad9-4a05-8db9-e794b29d5e97",
    "outputId": "8d77a5ea-4e15-4fd0-a7a2-19d20cf46aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total clips created: 1260\n",
      "Minimum samples per class: 50\n",
      "Dataset splits: Train=1007, Val=127, Test=126\n",
      "Using pre-trained TimeSformer weights.\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12a6ad8d6ca4858bca786d244156fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/487M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1512' max='2520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1512/2520 28:03 < 18:43, 0.90 it/s, Epoch 12/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Top1 Accuracy</th>\n",
       "      <th>Top5 Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.160200</td>\n",
       "      <td>1.345165</td>\n",
       "      <td>0.803150</td>\n",
       "      <td>0.976378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.038600</td>\n",
       "      <td>1.080723</td>\n",
       "      <td>0.842520</td>\n",
       "      <td>0.984252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.824300</td>\n",
       "      <td>1.030909</td>\n",
       "      <td>0.858268</td>\n",
       "      <td>0.976378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.722700</td>\n",
       "      <td>1.023571</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.968504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.674800</td>\n",
       "      <td>1.013771</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.976378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.652200</td>\n",
       "      <td>1.010637</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.952756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.642400</td>\n",
       "      <td>1.013888</td>\n",
       "      <td>0.889764</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>1.026655</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.634900</td>\n",
       "      <td>1.031386</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.952756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.633300</td>\n",
       "      <td>1.036304</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.952756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.632200</td>\n",
       "      <td>1.032573</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.952756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.631500</td>\n",
       "      <td>1.034936</td>\n",
       "      <td>0.866142</td>\n",
       "      <td>0.952756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL TEST RESULTS ===\n",
      "Test Top-1 Accuracy: 0.8492\n",
      "Test Top-5 Accuracy: 1.0000\n",
      "Test Loss: 1.0259\n",
      "Training complete. Model and results saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_top1_accuracy': 0.8492063492063492,\n",
       " 'eval_top5_accuracy': 1.0,\n",
       " 'eval_loss': 1.025852084159851,\n",
       " 'eval_runtime': 8.7083,\n",
       " 'eval_samples_per_second': 14.469,\n",
       " 'eval_steps_per_second': 1.837,\n",
       " 'epoch': 12.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_with_trainer(\n",
    "        data_dir=\"/content/drive/MyDrive/action-recognition-vit/HMDB_simp\",\n",
    "        epochs=20,\n",
    "        batch_size=8,\n",
    "        learning_rate=0.005\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HLpCYRzp9zrc",
   "metadata": {
    "id": "HLpCYRzp9zrc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "26e75539eff64aa28615f48416c4375a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbc533729c1d4142b5f74c35ca1f4f22",
      "placeholder": "​",
      "style": "IPY_MODEL_699626548c9e4daaaacfbf2879b9b0dd",
      "value": " 487M/487M [00:01&lt;00:00, 454MB/s]"
     }
    },
    "5a7d6eb7951d418bb938e9fee63e556d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "699626548c9e4daaaacfbf2879b9b0dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ec4203f2ee6456baf29b8ded748c5d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f1f56e383484946a401535d4613700c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9bab4f908db94f2eb05ca675ced2c4d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6959653d57341269a75d4266b6dc9ca",
      "max": 486911728,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7f1f56e383484946a401535d4613700c",
      "value": 486911728
     }
    },
    "9d6578408e744c219653d00e71f257ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6959653d57341269a75d4266b6dc9ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbc533729c1d4142b5f74c35ca1f4f22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d12a6ad8d6ca4858bca786d244156fa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e363cbca60394b57a51911a83b5a21b0",
       "IPY_MODEL_9bab4f908db94f2eb05ca675ced2c4d2",
       "IPY_MODEL_26e75539eff64aa28615f48416c4375a"
      ],
      "layout": "IPY_MODEL_9d6578408e744c219653d00e71f257ad"
     }
    },
    "e363cbca60394b57a51911a83b5a21b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ec4203f2ee6456baf29b8ded748c5d8",
      "placeholder": "​",
      "style": "IPY_MODEL_5a7d6eb7951d418bb938e9fee63e556d",
      "value": "model.safetensors: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
